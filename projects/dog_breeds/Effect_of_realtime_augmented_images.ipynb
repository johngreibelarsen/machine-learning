{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Test image augmentation effect on a typical CNN for classifying Dog Breeds \n",
    "\n",
    "- 1) First we will create a CNN _from scratch_ and test its accuracy against 50 different dog breeds\n",
    "- 2) Second we will manually augment data by physically creating augmented images on disk and then train against this data set to identify any changes to accuracy and overfitting\n",
    "- 3) Third we will use real-time data augmentation and see how that compares against point 2\n",
    "\n",
    "\n",
    "Note:\n",
    "That random chance presents an low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer 1 in 50 times, that is, in 2%.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Create realtime augmented images on the fly\n",
    "\n",
    "In the code block below we will load existing images and manipulate each image into an additional images in memory. Each images may be scaled, rotated, mirrored and/or undergo linear translation in order to obtain a better invariant representation. \n",
    "\n",
    "So what to expect: by using augmentation we expect better performance by having more data to train on and better statistical invariant data --> better at generalising and less overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from sklearn.datasets import load_files \n",
    "from keras.utils import np_utils\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "noOfBreeds = 10\n",
    "batchSize = 1\n",
    "\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), noOfBreeds)\n",
    "    return dog_files, dog_targets, np.array(data['target'])\n",
    "\n",
    "train_files, train_targets, train_targets_index = load_dataset('dogImages10/train')\n",
    "valid_files, valid_targets, valid_targets_index = load_dataset('dogImages10/valid')\n",
    "test_files, test_targets, test_targets_index  = load_dataset('dogImages10/test')\n",
    "# load list of dog names, 17 is due to the length of the string 'dogImages50/train'\n",
    "dog_names = [item[17:-1] for item in sorted(glob(\"dogImages10/train/*/\"))]\n",
    "\n",
    "train_data_generator = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    width_shift_range=0.15,  # randomly shift images horizontally (15% of total width)\n",
    "    height_shift_range=0.15,  # randomly shift images vertically (15% of total height)\n",
    "    rotation_range=45,  # degree range for random rotations\n",
    "    zoom_range=0.2, # range for random zoom [1-zoom_range, 1+zoom_range]\n",
    "    horizontal_flip=True, # randomly flip images horizontally\n",
    "    fill_mode='nearest') \n",
    "\n",
    "valid_data_generator = ImageDataGenerator(rescale=1.0/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!\n",
    "\n",
    "And finally we rescale the images by dividing every pixel in every image by 255. And why scale in the first place:\n",
    "\n",
    "1. Treat all images in the same manner: some images are high pixel range, some are low pixel range. The images are all sharing the same model, weights and learning rate. The high range image tends to create stronger loss while low range create weak loss, the sum of them will all contribute the back propagation update. But for visual understanding, you care about the contour more than how strong is the contrast as long as the contour is reserved. Scaling every images to the same range [0,1] will make images contributes more evenly to the total loss. In other words, a high pixel range cat image has one vote, a low pixel range cat image has one vote, a high pixel range dog image has one vote, a low pixel range dog image has one vote... this is more like what we expect for training a model for dog/cat image classifier. Without scaling, the high pixel range images will have large amount of votes to determine how to update weights. For example, black/white cat image could be higher pixel range than pure black cat image, but it just doesn't mean black/white cat image is more important for training.\n",
    "2. Using typical learning rate: when we reference learning rate from other's work, we can directly reference to their learning rate if both works do the scaling preprocessing over images data set. Otherwise, higher pixel range image results higher loss and should use smaller learning rate, lower pixel range image will need larger learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:01<00:00, 99.17it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 65.14it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 139.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "train_data_generator.fit(train_tensors)\n",
    "valid_data_generator.fit(valid_tensors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_85 (Conv2D)           (None, 224, 224, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_85 (MaxPooling (None, 111, 111, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_86 (Conv2D)           (None, 111, 111, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_86 (MaxPooling (None, 55, 55, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_87 (Conv2D)           (None, 55, 55, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_87 (MaxPooling (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_88 (Conv2D)           (None, 27, 27, 128)       32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_88 (MaxPooling (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 21632)             0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 64)                1384512   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 1,428,602\n",
      "Trainable params: 1,428,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D(pool_size=4, strides=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=3, strides=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))    \n",
    "model.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "#model.add(GlobalAveragePooling2D())   \n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.  \n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 99/100 [============================>.] - ETA: 95s - loss: 2.2562 - acc: 0.0000e+ - ETA: 52s - loss: 2.1173 - acc: 0.2500   - ETA: 37s - loss: 2.0452 - acc: 0.33 - ETA: 30s - loss: 2.1729 - acc: 0.25 - ETA: 26s - loss: 2.2169 - acc: 0.20 - ETA: 23s - loss: 2.2741 - acc: 0.16 - ETA: 21s - loss: 2.2952 - acc: 0.14 - ETA: 19s - loss: 2.2823 - acc: 0.12 - ETA: 18s - loss: 2.2850 - acc: 0.11 - ETA: 17s - loss: 2.3075 - acc: 0.10 - ETA: 16s - loss: 2.3353 - acc: 0.09 - ETA: 15s - loss: 2.3274 - acc: 0.08 - ETA: 14s - loss: 2.3378 - acc: 0.07 - ETA: 14s - loss: 2.3355 - acc: 0.07 - ETA: 13s - loss: 2.3469 - acc: 0.06 - ETA: 13s - loss: 2.3495 - acc: 0.06 - ETA: 12s - loss: 2.3515 - acc: 0.05 - ETA: 12s - loss: 2.3480 - acc: 0.08 - ETA: 12s - loss: 2.3466 - acc: 0.07 - ETA: 11s - loss: 2.3433 - acc: 0.10 - ETA: 11s - loss: 2.3410 - acc: 0.09 - ETA: 11s - loss: 2.3424 - acc: 0.09 - ETA: 11s - loss: 2.3443 - acc: 0.08 - ETA: 10s - loss: 2.3400 - acc: 0.08 - ETA: 10s - loss: 2.3415 - acc: 0.08 - ETA: 10s - loss: 2.3411 - acc: 0.07 - ETA: 10s - loss: 2.3396 - acc: 0.07 - ETA: 9s - loss: 2.3371 - acc: 0.0893 - ETA: 9s - loss: 2.3386 - acc: 0.086 - ETA: 9s - loss: 2.3373 - acc: 0.083 - ETA: 9s - loss: 2.3352 - acc: 0.080 - ETA: 9s - loss: 2.3368 - acc: 0.078 - ETA: 8s - loss: 2.3371 - acc: 0.075 - ETA: 8s - loss: 2.3374 - acc: 0.073 - ETA: 8s - loss: 2.3361 - acc: 0.085 - ETA: 8s - loss: 2.3355 - acc: 0.083 - ETA: 8s - loss: 2.3340 - acc: 0.081 - ETA: 8s - loss: 2.3327 - acc: 0.078 - ETA: 7s - loss: 2.3315 - acc: 0.089 - ETA: 7s - loss: 2.3311 - acc: 0.087 - ETA: 7s - loss: 2.3301 - acc: 0.085 - ETA: 7s - loss: 2.3280 - acc: 0.095 - ETA: 7s - loss: 2.3276 - acc: 0.093 - ETA: 7s - loss: 2.3267 - acc: 0.102 - ETA: 6s - loss: 2.3275 - acc: 0.100 - ETA: 6s - loss: 2.3288 - acc: 0.097 - ETA: 6s - loss: 2.3288 - acc: 0.095 - ETA: 6s - loss: 2.3288 - acc: 0.093 - ETA: 6s - loss: 2.3285 - acc: 0.091 - ETA: 6s - loss: 2.3278 - acc: 0.090 - ETA: 6s - loss: 2.3280 - acc: 0.088 - ETA: 5s - loss: 2.3271 - acc: 0.086 - ETA: 5s - loss: 2.3266 - acc: 0.094 - ETA: 5s - loss: 2.3257 - acc: 0.092 - ETA: 5s - loss: 2.3258 - acc: 0.090 - ETA: 5s - loss: 2.3255 - acc: 0.089 - ETA: 5s - loss: 2.3255 - acc: 0.087 - ETA: 5s - loss: 2.3251 - acc: 0.086 - ETA: 5s - loss: 2.3251 - acc: 0.084 - ETA: 4s - loss: 2.3250 - acc: 0.083 - ETA: 4s - loss: 2.3244 - acc: 0.090 - ETA: 4s - loss: 2.3239 - acc: 0.096 - ETA: 4s - loss: 2.3228 - acc: 0.103 - ETA: 4s - loss: 2.3232 - acc: 0.101 - ETA: 4s - loss: 2.3233 - acc: 0.100 - ETA: 4s - loss: 2.3231 - acc: 0.098 - ETA: 3s - loss: 2.3231 - acc: 0.097 - ETA: 3s - loss: 2.3229 - acc: 0.095 - ETA: 3s - loss: 2.3231 - acc: 0.094 - ETA: 3s - loss: 2.3228 - acc: 0.092 - ETA: 3s - loss: 2.3227 - acc: 0.091 - ETA: 3s - loss: 2.3224 - acc: 0.090 - ETA: 3s - loss: 2.3225 - acc: 0.089 - ETA: 3s - loss: 2.3225 - acc: 0.087 - ETA: 2s - loss: 2.3222 - acc: 0.086 - ETA: 2s - loss: 2.3222 - acc: 0.085 - ETA: 2s - loss: 2.3221 - acc: 0.084 - ETA: 2s - loss: 2.3220 - acc: 0.083 - ETA: 2s - loss: 2.3218 - acc: 0.082 - ETA: 2s - loss: 2.3216 - acc: 0.081 - ETA: 2s - loss: 2.3215 - acc: 0.080 - ETA: 2s - loss: 2.3212 - acc: 0.079 - ETA: 2s - loss: 2.3210 - acc: 0.078 - ETA: 1s - loss: 2.3207 - acc: 0.077 - ETA: 1s - loss: 2.3203 - acc: 0.082 - ETA: 1s - loss: 2.3202 - acc: 0.081 - ETA: 1s - loss: 2.3199 - acc: 0.080 - ETA: 1s - loss: 2.3195 - acc: 0.079 - ETA: 1s - loss: 2.3196 - acc: 0.078 - ETA: 1s - loss: 2.3196 - acc: 0.077 - ETA: 1s - loss: 2.3193 - acc: 0.076 - ETA: 0s - loss: 2.3191 - acc: 0.076 - ETA: 0s - loss: 2.3188 - acc: 0.080 - ETA: 0s - loss: 2.3187 - acc: 0.079 - ETA: 0s - loss: 2.3186 - acc: 0.078 - ETA: 0s - loss: 2.3184 - acc: 0.078 - ETA: 0s - loss: 2.3180 - acc: 0.082 - ETA: 0s - loss: 2.3182 - acc: 0.081 - ETA: 0s - loss: 2.3179 - acc: 0.0808Epoch 00000: val_loss improved from inf to 2.30215, saving model to saved_models/weights.best.from_scratch.real.aug.v9.hdf5\n",
      "100/100 [==============================] - 12s - loss: 2.3181 - acc: 0.0800 - val_loss: 2.3021 - val_acc: 0.1000\n",
      "Epoch 2/10\n",
      " 99/100 [============================>.] - ETA: 9s - loss: 2.2877 - acc: 0.0000e+0 - ETA: 9s - loss: 2.3132 - acc: 0.0000e+0 - ETA: 9s - loss: 2.3042 - acc: 0.1667    - ETA: 10s - loss: 2.3028 - acc: 0.12 - ETA: 10s - loss: 2.3008 - acc: 0.10 - ETA: 9s - loss: 2.3008 - acc: 0.0833 - ETA: 9s - loss: 2.2975 - acc: 0.071 - ETA: 9s - loss: 2.2931 - acc: 0.125 - ETA: 9s - loss: 2.2972 - acc: 0.111 - ETA: 9s - loss: 2.2965 - acc: 0.100 - ETA: 9s - loss: 2.2950 - acc: 0.090 - ETA: 9s - loss: 2.3043 - acc: 0.083 - ETA: 9s - loss: 2.3058 - acc: 0.076 - ETA: 9s - loss: 2.3000 - acc: 0.107 - ETA: 8s - loss: 2.3013 - acc: 0.100 - ETA: 8s - loss: 2.2988 - acc: 0.125 - ETA: 8s - loss: 2.3014 - acc: 0.117 - ETA: 8s - loss: 2.3044 - acc: 0.111 - ETA: 8s - loss: 2.3016 - acc: 0.105 - ETA: 8s - loss: 2.3004 - acc: 0.125 - ETA: 8s - loss: 2.3008 - acc: 0.119 - ETA: 8s - loss: 2.3019 - acc: 0.113 - ETA: 8s - loss: 2.3015 - acc: 0.108 - ETA: 8s - loss: 2.3029 - acc: 0.104 - ETA: 7s - loss: 2.3022 - acc: 0.100 - ETA: 7s - loss: 2.3037 - acc: 0.096 - ETA: 7s - loss: 2.3047 - acc: 0.092 - ETA: 7s - loss: 2.3052 - acc: 0.089 - ETA: 7s - loss: 2.3059 - acc: 0.086 - ETA: 7s - loss: 2.3083 - acc: 0.083 - ETA: 7s - loss: 2.3086 - acc: 0.080 - ETA: 7s - loss: 2.3084 - acc: 0.078 - ETA: 7s - loss: 2.3082 - acc: 0.075 - ETA: 6s - loss: 2.3083 - acc: 0.073 - ETA: 6s - loss: 2.3077 - acc: 0.085 - ETA: 6s - loss: 2.3070 - acc: 0.083 - ETA: 6s - loss: 2.3077 - acc: 0.081 - ETA: 6s - loss: 2.3078 - acc: 0.078 - ETA: 6s - loss: 2.3083 - acc: 0.076 - ETA: 6s - loss: 2.3089 - acc: 0.075 - ETA: 6s - loss: 2.3090 - acc: 0.073 - ETA: 6s - loss: 2.3090 - acc: 0.071 - ETA: 6s - loss: 2.3087 - acc: 0.069 - ETA: 5s - loss: 2.3084 - acc: 0.068 - ETA: 5s - loss: 2.3085 - acc: 0.077 - ETA: 5s - loss: 2.3082 - acc: 0.087 - ETA: 5s - loss: 2.3083 - acc: 0.085 - ETA: 5s - loss: 2.3079 - acc: 0.083 - ETA: 5s - loss: 2.3081 - acc: 0.081 - ETA: 5s - loss: 2.3074 - acc: 0.080 - ETA: 5s - loss: 2.3079 - acc: 0.078 - ETA: 5s - loss: 2.3081 - acc: 0.076 - ETA: 4s - loss: 2.3083 - acc: 0.084 - ETA: 4s - loss: 2.3083 - acc: 0.092 - ETA: 4s - loss: 2.3089 - acc: 0.090 - ETA: 4s - loss: 2.3089 - acc: 0.089 - ETA: 4s - loss: 2.3088 - acc: 0.096 - ETA: 4s - loss: 2.3086 - acc: 0.094 - ETA: 4s - loss: 2.3086 - acc: 0.093 - ETA: 4s - loss: 2.3080 - acc: 0.100 - ETA: 4s - loss: 2.3079 - acc: 0.098 - ETA: 4s - loss: 2.3078 - acc: 0.096 - ETA: 3s - loss: 2.3077 - acc: 0.095 - ETA: 3s - loss: 2.3082 - acc: 0.093 - ETA: 3s - loss: 2.3081 - acc: 0.092 - ETA: 3s - loss: 2.3080 - acc: 0.090 - ETA: 3s - loss: 2.3078 - acc: 0.097 - ETA: 3s - loss: 2.3076 - acc: 0.095 - ETA: 3s - loss: 2.3071 - acc: 0.094 - ETA: 3s - loss: 2.3071 - acc: 0.092 - ETA: 3s - loss: 2.3070 - acc: 0.091 - ETA: 2s - loss: 2.3067 - acc: 0.090 - ETA: 2s - loss: 2.3068 - acc: 0.089 - ETA: 2s - loss: 2.3065 - acc: 0.087 - ETA: 2s - loss: 2.3065 - acc: 0.093 - ETA: 2s - loss: 2.3078 - acc: 0.092 - ETA: 2s - loss: 2.3077 - acc: 0.090 - ETA: 2s - loss: 2.3071 - acc: 0.089 - ETA: 2s - loss: 2.3077 - acc: 0.088 - ETA: 2s - loss: 2.3064 - acc: 0.093 - ETA: 2s - loss: 2.3057 - acc: 0.098 - ETA: 1s - loss: 2.3075 - acc: 0.097 - ETA: 1s - loss: 2.3084 - acc: 0.096 - ETA: 1s - loss: 2.3094 - acc: 0.095 - ETA: 1s - loss: 2.3092 - acc: 0.094 - ETA: 1s - loss: 2.3092 - acc: 0.093 - ETA: 1s - loss: 2.3097 - acc: 0.092 - ETA: 1s - loss: 2.3095 - acc: 0.090 - ETA: 1s - loss: 2.3089 - acc: 0.095 - ETA: 1s - loss: 2.3093 - acc: 0.094 - ETA: 0s - loss: 2.3099 - acc: 0.093 - ETA: 0s - loss: 2.3101 - acc: 0.092 - ETA: 0s - loss: 2.3100 - acc: 0.091 - ETA: 0s - loss: 2.3097 - acc: 0.095 - ETA: 0s - loss: 2.3099 - acc: 0.094 - ETA: 0s - loss: 2.3098 - acc: 0.099 - ETA: 0s - loss: 2.3095 - acc: 0.103 - ETA: 0s - loss: 2.3096 - acc: 0.102 - ETA: 0s - loss: 2.3093 - acc: 0.1010Epoch 00001: val_loss improved from 2.30215 to 2.30152, saving model to saved_models/weights.best.from_scratch.real.aug.v9.hdf5\n",
      "100/100 [==============================] - 11s - loss: 2.3084 - acc: 0.1050 - val_loss: 2.3015 - val_acc: 0.1200\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 9s - loss: 2.2959 - acc: 0.0000e+0 - ETA: 9s - loss: 2.2698 - acc: 0.0000e+0 - ETA: 9s - loss: 2.2586 - acc: 0.0000e+0 - ETA: 9s - loss: 2.2634 - acc: 0.0000e+0 - ETA: 9s - loss: 2.2713 - acc: 0.0000e+0 - ETA: 9s - loss: 2.2776 - acc: 0.0000e+0 - ETA: 9s - loss: 2.2655 - acc: 0.0000e+0 - ETA: 9s - loss: 2.2680 - acc: 0.0000e+0 - ETA: 9s - loss: 2.2419 - acc: 0.0556    - ETA: 9s - loss: 2.2484 - acc: 0.050 - ETA: 9s - loss: 2.2708 - acc: 0.045 - ETA: 9s - loss: 2.2656 - acc: 0.083 - ETA: 9s - loss: 2.2748 - acc: 0.076 - ETA: 8s - loss: 2.2759 - acc: 0.071 - ETA: 8s - loss: 2.2596 - acc: 0.100 - ETA: 8s - loss: 2.2741 - acc: 0.093 - ETA: 8s - loss: 2.2794 - acc: 0.088 - ETA: 8s - loss: 2.2976 - acc: 0.083 - ETA: 8s - loss: 2.2893 - acc: 0.078 - ETA: 8s - loss: 2.3023 - acc: 0.075 - ETA: 8s - loss: 2.3040 - acc: 0.071 - ETA: 8s - loss: 2.2926 - acc: 0.113 - ETA: 8s - loss: 2.2893 - acc: 0.108 - ETA: 8s - loss: 2.2803 - acc: 0.125 - ETA: 7s - loss: 2.2878 - acc: 0.120 - ETA: 7s - loss: 2.2891 - acc: 0.115 - ETA: 7s - loss: 2.2917 - acc: 0.111 - ETA: 7s - loss: 2.2947 - acc: 0.107 - ETA: 7s - loss: 2.2932 - acc: 0.103 - ETA: 7s - loss: 2.2976 - acc: 0.100 - ETA: 7s - loss: 2.2995 - acc: 0.096 - ETA: 7s - loss: 2.2988 - acc: 0.093 - ETA: 7s - loss: 2.2982 - acc: 0.090 - ETA: 6s - loss: 2.3017 - acc: 0.088 - ETA: 6s - loss: 2.3018 - acc: 0.085 - ETA: 6s - loss: 2.3029 - acc: 0.083 - ETA: 6s - loss: 2.3000 - acc: 0.094 - ETA: 6s - loss: 2.2995 - acc: 0.092 - ETA: 6s - loss: 2.2998 - acc: 0.089 - ETA: 6s - loss: 2.3005 - acc: 0.087 - ETA: 6s - loss: 2.3000 - acc: 0.085 - ETA: 6s - loss: 2.3008 - acc: 0.083 - ETA: 5s - loss: 2.3005 - acc: 0.093 - ETA: 5s - loss: 2.2975 - acc: 0.102 - ETA: 5s - loss: 2.2993 - acc: 0.100 - ETA: 5s - loss: 2.2982 - acc: 0.097 - ETA: 5s - loss: 2.2993 - acc: 0.106 - ETA: 5s - loss: 2.2999 - acc: 0.104 - ETA: 5s - loss: 2.3004 - acc: 0.102 - ETA: 5s - loss: 2.2992 - acc: 0.100 - ETA: 5s - loss: 2.2948 - acc: 0.117 - ETA: 5s - loss: 2.2985 - acc: 0.115 - ETA: 4s - loss: 2.3000 - acc: 0.113 - ETA: 4s - loss: 2.3001 - acc: 0.111 - ETA: 4s - loss: 2.2995 - acc: 0.109 - ETA: 4s - loss: 2.3010 - acc: 0.107 - ETA: 4s - loss: 2.3011 - acc: 0.105 - ETA: 4s - loss: 2.3037 - acc: 0.103 - ETA: 4s - loss: 2.3041 - acc: 0.101 - ETA: 4s - loss: 2.3042 - acc: 0.100 - ETA: 4s - loss: 2.3044 - acc: 0.098 - ETA: 3s - loss: 2.3052 - acc: 0.096 - ETA: 3s - loss: 2.3055 - acc: 0.095 - ETA: 3s - loss: 2.3053 - acc: 0.093 - ETA: 3s - loss: 2.3049 - acc: 0.092 - ETA: 3s - loss: 2.3052 - acc: 0.090 - ETA: 3s - loss: 2.3058 - acc: 0.089 - ETA: 3s - loss: 2.3055 - acc: 0.088 - ETA: 3s - loss: 2.3052 - acc: 0.087 - ETA: 3s - loss: 2.3047 - acc: 0.085 - ETA: 3s - loss: 2.3039 - acc: 0.091 - ETA: 2s - loss: 2.3043 - acc: 0.090 - ETA: 2s - loss: 2.3059 - acc: 0.089 - ETA: 2s - loss: 2.3059 - acc: 0.087 - ETA: 2s - loss: 2.3045 - acc: 0.093 - ETA: 2s - loss: 2.3046 - acc: 0.092 - ETA: 2s - loss: 2.3043 - acc: 0.097 - ETA: 2s - loss: 2.3060 - acc: 0.096 - ETA: 2s - loss: 2.3062 - acc: 0.094 - ETA: 2s - loss: 2.3064 - acc: 0.093 - ETA: 1s - loss: 2.3065 - acc: 0.092 - ETA: 1s - loss: 2.3068 - acc: 0.091 - ETA: 1s - loss: 2.3069 - acc: 0.090 - ETA: 1s - loss: 2.3072 - acc: 0.089 - ETA: 1s - loss: 2.3076 - acc: 0.088 - ETA: 1s - loss: 2.3078 - acc: 0.087 - ETA: 1s - loss: 2.3079 - acc: 0.086 - ETA: 1s - loss: 2.3079 - acc: 0.085 - ETA: 1s - loss: 2.3080 - acc: 0.084 - ETA: 1s - loss: 2.3079 - acc: 0.083 - ETA: 0s - loss: 2.3081 - acc: 0.087 - ETA: 0s - loss: 2.3081 - acc: 0.087 - ETA: 0s - loss: 2.3080 - acc: 0.086 - ETA: 0s - loss: 2.3079 - acc: 0.085 - ETA: 0s - loss: 2.3080 - acc: 0.084 - ETA: 0s - loss: 2.3081 - acc: 0.083 - ETA: 0s - loss: 2.3080 - acc: 0.087 - ETA: 0s - loss: 2.3079 - acc: 0.086 - ETA: 0s - loss: 2.3077 - acc: 0.0909Epoch 00002: val_loss did not improve\n",
      "100/100 [==============================] - 11s - loss: 2.3077 - acc: 0.0900 - val_loss: 2.3023 - val_acc: 0.1200\n",
      "Epoch 4/10\n",
      " 99/100 [============================>.] - ETA: 8s - loss: 2.3058 - acc: 0.0000e+0 - ETA: 9s - loss: 2.2990 - acc: 0.0000e+0 - ETA: 9s - loss: 2.3056 - acc: 0.0000e+0 - ETA: 9s - loss: 2.2969 - acc: 0.1250    - ETA: 9s - loss: 2.2984 - acc: 0.100 - ETA: 9s - loss: 2.2991 - acc: 0.083 - ETA: 9s - loss: 2.2995 - acc: 0.071 - ETA: 9s - loss: 2.3012 - acc: 0.062 - ETA: 9s - loss: 2.3028 - acc: 0.055 - ETA: 9s - loss: 2.3011 - acc: 0.100 - ETA: 9s - loss: 2.3033 - acc: 0.090 - ETA: 9s - loss: 2.3038 - acc: 0.083 - ETA: 9s - loss: 2.3021 - acc: 0.115 - ETA: 9s - loss: 2.3018 - acc: 0.107 - ETA: 8s - loss: 2.3013 - acc: 0.100 - ETA: 8s - loss: 2.3027 - acc: 0.093 - ETA: 8s - loss: 2.3034 - acc: 0.088 - ETA: 8s - loss: 2.3031 - acc: 0.083 - ETA: 8s - loss: 2.3022 - acc: 0.078 - ETA: 8s - loss: 2.3020 - acc: 0.100 - ETA: 8s - loss: 2.3018 - acc: 0.095 - ETA: 8s - loss: 2.3032 - acc: 0.090 - ETA: 8s - loss: 2.3020 - acc: 0.108 - ETA: 8s - loss: 2.3030 - acc: 0.104 - ETA: 7s - loss: 2.3022 - acc: 0.100 - ETA: 7s - loss: 2.3033 - acc: 0.115 - ETA: 7s - loss: 2.3029 - acc: 0.111 - ETA: 7s - loss: 2.3014 - acc: 0.107 - ETA: 7s - loss: 2.2995 - acc: 0.103 - ETA: 7s - loss: 2.3039 - acc: 0.100 - ETA: 7s - loss: 2.3026 - acc: 0.096 - ETA: 7s - loss: 2.3027 - acc: 0.093 - ETA: 7s - loss: 2.3020 - acc: 0.090 - ETA: 6s - loss: 2.3032 - acc: 0.088 - ETA: 6s - loss: 2.3038 - acc: 0.085 - ETA: 6s - loss: 2.3034 - acc: 0.097 - ETA: 6s - loss: 2.3039 - acc: 0.094 - ETA: 6s - loss: 2.3031 - acc: 0.105 - ETA: 6s - loss: 2.3026 - acc: 0.102 - ETA: 6s - loss: 2.3032 - acc: 0.100 - ETA: 6s - loss: 2.3030 - acc: 0.109 - ETA: 6s - loss: 2.3039 - acc: 0.107 - ETA: 6s - loss: 2.3045 - acc: 0.104 - ETA: 5s - loss: 2.3055 - acc: 0.102 - ETA: 5s - loss: 2.3052 - acc: 0.111 - ETA: 5s - loss: 2.3064 - acc: 0.108 - ETA: 5s - loss: 2.3064 - acc: 0.106 - ETA: 5s - loss: 2.3064 - acc: 0.104 - ETA: 5s - loss: 2.3071 - acc: 0.102 - ETA: 5s - loss: 2.3070 - acc: 0.100 - ETA: 5s - loss: 2.3078 - acc: 0.098 - ETA: 5s - loss: 2.3080 - acc: 0.096 - ETA: 5s - loss: 2.3075 - acc: 0.094 - ETA: 4s - loss: 2.3075 - acc: 0.092 - ETA: 4s - loss: 2.3077 - acc: 0.090 - ETA: 4s - loss: 2.3077 - acc: 0.089 - ETA: 4s - loss: 2.3078 - acc: 0.087 - ETA: 4s - loss: 2.3075 - acc: 0.086 - ETA: 4s - loss: 2.3073 - acc: 0.084 - ETA: 4s - loss: 2.3068 - acc: 0.091 - ETA: 4s - loss: 2.3073 - acc: 0.090 - ETA: 4s - loss: 2.3069 - acc: 0.096 - ETA: 3s - loss: 2.3070 - acc: 0.095 - ETA: 3s - loss: 2.3067 - acc: 0.101 - ETA: 3s - loss: 2.3072 - acc: 0.100 - ETA: 3s - loss: 2.3069 - acc: 0.098 - ETA: 3s - loss: 2.3072 - acc: 0.097 - ETA: 3s - loss: 2.3079 - acc: 0.095 - ETA: 3s - loss: 2.3081 - acc: 0.094 - ETA: 3s - loss: 2.3082 - acc: 0.092 - ETA: 3s - loss: 2.3081 - acc: 0.091 - ETA: 3s - loss: 2.3080 - acc: 0.090 - ETA: 2s - loss: 2.3079 - acc: 0.089 - ETA: 2s - loss: 2.3079 - acc: 0.087 - ETA: 2s - loss: 2.3079 - acc: 0.086 - ETA: 2s - loss: 2.3081 - acc: 0.085 - ETA: 2s - loss: 2.3081 - acc: 0.084 - ETA: 2s - loss: 2.3081 - acc: 0.083 - ETA: 2s - loss: 2.3080 - acc: 0.082 - ETA: 2s - loss: 2.3083 - acc: 0.081 - ETA: 2s - loss: 2.3082 - acc: 0.080 - ETA: 1s - loss: 2.3082 - acc: 0.079 - ETA: 1s - loss: 2.3084 - acc: 0.078 - ETA: 1s - loss: 2.3081 - acc: 0.083 - ETA: 1s - loss: 2.3081 - acc: 0.082 - ETA: 1s - loss: 2.3078 - acc: 0.081 - ETA: 1s - loss: 2.3076 - acc: 0.080 - ETA: 1s - loss: 2.3074 - acc: 0.079 - ETA: 1s - loss: 2.3074 - acc: 0.078 - ETA: 1s - loss: 2.3073 - acc: 0.077 - ETA: 0s - loss: 2.3072 - acc: 0.076 - ETA: 0s - loss: 2.3073 - acc: 0.076 - ETA: 0s - loss: 2.3071 - acc: 0.075 - ETA: 0s - loss: 2.3070 - acc: 0.074 - ETA: 0s - loss: 2.3073 - acc: 0.073 - ETA: 0s - loss: 2.3075 - acc: 0.072 - ETA: 0s - loss: 2.3074 - acc: 0.072 - ETA: 0s - loss: 2.3082 - acc: 0.071 - ETA: 0s - loss: 2.3081 - acc: 0.0707Epoch 00003: val_loss did not improve\n",
      "100/100 [==============================] - 11s - loss: 2.3081 - acc: 0.0700 - val_loss: 2.3038 - val_acc: 0.0800\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 10s - loss: 2.3010 - acc: 0.0000e+ - ETA: 10s - loss: 2.3186 - acc: 0.0000e+ - ETA: 10s - loss: 2.3095 - acc: 0.0000e+ - ETA: 10s - loss: 2.3039 - acc: 0.1250   - ETA: 10s - loss: 2.3028 - acc: 0.10 - ETA: 10s - loss: 2.3042 - acc: 0.08 - ETA: 9s - loss: 2.3027 - acc: 0.0714 - ETA: 9s - loss: 2.2971 - acc: 0.125 - ETA: 9s - loss: 2.2929 - acc: 0.111 - ETA: 9s - loss: 2.2931 - acc: 0.100 - ETA: 9s - loss: 2.2920 - acc: 0.090 - ETA: 9s - loss: 2.2941 - acc: 0.083 - ETA: 9s - loss: 2.2933 - acc: 0.115 - ETA: 9s - loss: 2.2928 - acc: 0.142 - ETA: 9s - loss: 2.2944 - acc: 0.133 - ETA: 8s - loss: 2.2996 - acc: 0.125 - ETA: 8s - loss: 2.3037 - acc: 0.117 - ETA: 8s - loss: 2.3031 - acc: 0.111 - ETA: 8s - loss: 2.3020 - acc: 0.105 - ETA: 8s - loss: 2.2983 - acc: 0.100 - ETA: 8s - loss: 2.3031 - acc: 0.095 - ETA: 8s - loss: 2.3027 - acc: 0.090 - ETA: 8s - loss: 2.3033 - acc: 0.087 - ETA: 8s - loss: 2.3058 - acc: 0.083 - ETA: 7s - loss: 2.3037 - acc: 0.100 - ETA: 7s - loss: 2.3026 - acc: 0.115 - ETA: 7s - loss: 2.3012 - acc: 0.129 - ETA: 7s - loss: 2.3016 - acc: 0.125 - ETA: 7s - loss: 2.3038 - acc: 0.120 - ETA: 7s - loss: 2.3039 - acc: 0.116 - ETA: 7s - loss: 2.3036 - acc: 0.112 - ETA: 7s - loss: 2.3039 - acc: 0.109 - ETA: 7s - loss: 2.3044 - acc: 0.106 - ETA: 7s - loss: 2.3047 - acc: 0.102 - ETA: 6s - loss: 2.3044 - acc: 0.100 - ETA: 6s - loss: 2.3046 - acc: 0.097 - ETA: 6s - loss: 2.3045 - acc: 0.094 - ETA: 6s - loss: 2.3039 - acc: 0.092 - ETA: 6s - loss: 2.3009 - acc: 0.115 - ETA: 6s - loss: 2.3033 - acc: 0.112 - ETA: 6s - loss: 2.3029 - acc: 0.122 - ETA: 6s - loss: 2.3000 - acc: 0.131 - ETA: 6s - loss: 2.2987 - acc: 0.127 - ETA: 6s - loss: 2.2967 - acc: 0.125 - ETA: 5s - loss: 2.2991 - acc: 0.122 - ETA: 5s - loss: 2.3017 - acc: 0.119 - ETA: 5s - loss: 2.3017 - acc: 0.117 - ETA: 5s - loss: 2.3018 - acc: 0.114 - ETA: 5s - loss: 2.3014 - acc: 0.122 - ETA: 5s - loss: 2.3022 - acc: 0.120 - ETA: 5s - loss: 2.3023 - acc: 0.117 - ETA: 5s - loss: 2.3025 - acc: 0.115 - ETA: 5s - loss: 2.3018 - acc: 0.113 - ETA: 4s - loss: 2.3034 - acc: 0.111 - ETA: 4s - loss: 2.3044 - acc: 0.109 - ETA: 4s - loss: 2.3041 - acc: 0.107 - ETA: 4s - loss: 2.3041 - acc: 0.105 - ETA: 4s - loss: 2.3039 - acc: 0.112 - ETA: 4s - loss: 2.3033 - acc: 0.118 - ETA: 4s - loss: 2.3043 - acc: 0.116 - ETA: 4s - loss: 2.3040 - acc: 0.123 - ETA: 4s - loss: 2.3040 - acc: 0.121 - ETA: 4s - loss: 2.3042 - acc: 0.119 - ETA: 3s - loss: 2.3045 - acc: 0.117 - ETA: 3s - loss: 2.3049 - acc: 0.115 - ETA: 3s - loss: 2.3045 - acc: 0.113 - ETA: 3s - loss: 2.3040 - acc: 0.119 - ETA: 3s - loss: 2.3044 - acc: 0.117 - ETA: 3s - loss: 2.3046 - acc: 0.115 - ETA: 3s - loss: 2.3046 - acc: 0.114 - ETA: 3s - loss: 2.3050 - acc: 0.112 - ETA: 3s - loss: 2.3053 - acc: 0.111 - ETA: 2s - loss: 2.3058 - acc: 0.109 - ETA: 2s - loss: 2.3059 - acc: 0.108 - ETA: 2s - loss: 2.3059 - acc: 0.106 - ETA: 2s - loss: 2.3058 - acc: 0.105 - ETA: 2s - loss: 2.3046 - acc: 0.103 - ETA: 2s - loss: 2.3044 - acc: 0.102 - ETA: 2s - loss: 2.3039 - acc: 0.107 - ETA: 2s - loss: 2.3047 - acc: 0.106 - ETA: 2s - loss: 2.3039 - acc: 0.111 - ETA: 1s - loss: 2.3053 - acc: 0.109 - ETA: 1s - loss: 2.3055 - acc: 0.108 - ETA: 1s - loss: 2.3054 - acc: 0.107 - ETA: 1s - loss: 2.3058 - acc: 0.105 - ETA: 1s - loss: 2.3060 - acc: 0.104 - ETA: 1s - loss: 2.3062 - acc: 0.103 - ETA: 1s - loss: 2.3058 - acc: 0.102 - ETA: 1s - loss: 2.3060 - acc: 0.101 - ETA: 1s - loss: 2.3057 - acc: 0.100 - ETA: 0s - loss: 2.3055 - acc: 0.098 - ETA: 0s - loss: 2.3052 - acc: 0.097 - ETA: 0s - loss: 2.3055 - acc: 0.096 - ETA: 0s - loss: 2.3056 - acc: 0.095 - ETA: 0s - loss: 2.3055 - acc: 0.094 - ETA: 0s - loss: 2.3059 - acc: 0.093 - ETA: 0s - loss: 2.3058 - acc: 0.092 - ETA: 0s - loss: 2.3059 - acc: 0.091 - ETA: 0s - loss: 2.3061 - acc: 0.0909Epoch 00004: val_loss did not improve\n",
      "100/100 [==============================] - 11s - loss: 2.3063 - acc: 0.0900 - val_loss: 2.3080 - val_acc: 0.0400\n",
      "Epoch 6/10\n",
      " 99/100 [============================>.] - ETA: 11s - loss: 2.2890 - acc: 0.0000e+ - ETA: 11s - loss: 2.3007 - acc: 0.0000e+ - ETA: 11s - loss: 2.3028 - acc: 0.0000e+ - ETA: 11s - loss: 2.3002 - acc: 0.1250   - ETA: 11s - loss: 2.2986 - acc: 0.10 - ETA: 11s - loss: 2.3028 - acc: 0.08 - ETA: 11s - loss: 2.2998 - acc: 0.07 - ETA: 11s - loss: 2.2994 - acc: 0.06 - ETA: 10s - loss: 2.2991 - acc: 0.05 - ETA: 10s - loss: 2.3028 - acc: 0.05 - ETA: 10s - loss: 2.3007 - acc: 0.04 - ETA: 10s - loss: 2.3017 - acc: 0.04 - ETA: 10s - loss: 2.3002 - acc: 0.03 - ETA: 10s - loss: 2.3016 - acc: 0.03 - ETA: 10s - loss: 2.3018 - acc: 0.03 - ETA: 10s - loss: 2.3015 - acc: 0.03 - ETA: 10s - loss: 2.3019 - acc: 0.02 - ETA: 9s - loss: 2.3015 - acc: 0.0278 - ETA: 9s - loss: 2.3001 - acc: 0.052 - ETA: 9s - loss: 2.3006 - acc: 0.050 - ETA: 9s - loss: 2.3022 - acc: 0.047 - ETA: 9s - loss: 2.3029 - acc: 0.045 - ETA: 9s - loss: 2.3034 - acc: 0.043 - ETA: 9s - loss: 2.3024 - acc: 0.041 - ETA: 8s - loss: 2.3022 - acc: 0.040 - ETA: 8s - loss: 2.3020 - acc: 0.038 - ETA: 8s - loss: 2.3012 - acc: 0.055 - ETA: 8s - loss: 2.2999 - acc: 0.071 - ETA: 8s - loss: 2.2992 - acc: 0.069 - ETA: 8s - loss: 2.3011 - acc: 0.066 - ETA: 8s - loss: 2.3008 - acc: 0.064 - ETA: 8s - loss: 2.3017 - acc: 0.062 - ETA: 7s - loss: 2.3018 - acc: 0.060 - ETA: 7s - loss: 2.3018 - acc: 0.058 - ETA: 7s - loss: 2.3045 - acc: 0.057 - ETA: 7s - loss: 2.3026 - acc: 0.055 - ETA: 7s - loss: 2.3001 - acc: 0.054 - ETA: 7s - loss: 2.2977 - acc: 0.052 - ETA: 7s - loss: 2.3011 - acc: 0.051 - ETA: 7s - loss: 2.2966 - acc: 0.062 - ETA: 7s - loss: 2.2927 - acc: 0.073 - ETA: 6s - loss: 2.2934 - acc: 0.071 - ETA: 6s - loss: 2.2886 - acc: 0.069 - ETA: 6s - loss: 2.2861 - acc: 0.068 - ETA: 6s - loss: 2.2857 - acc: 0.066 - ETA: 6s - loss: 2.2840 - acc: 0.065 - ETA: 6s - loss: 2.2819 - acc: 0.063 - ETA: 6s - loss: 2.2844 - acc: 0.062 - ETA: 6s - loss: 2.2851 - acc: 0.061 - ETA: 5s - loss: 2.2841 - acc: 0.060 - ETA: 5s - loss: 2.2863 - acc: 0.058 - ETA: 5s - loss: 2.2842 - acc: 0.067 - ETA: 5s - loss: 2.2813 - acc: 0.066 - ETA: 5s - loss: 2.2858 - acc: 0.064 - ETA: 5s - loss: 2.2855 - acc: 0.063 - ETA: 5s - loss: 2.2789 - acc: 0.071 - ETA: 5s - loss: 2.2802 - acc: 0.070 - ETA: 4s - loss: 2.2836 - acc: 0.069 - ETA: 4s - loss: 2.2838 - acc: 0.067 - ETA: 4s - loss: 2.2826 - acc: 0.066 - ETA: 4s - loss: 2.2841 - acc: 0.065 - ETA: 4s - loss: 2.2837 - acc: 0.064 - ETA: 4s - loss: 2.2846 - acc: 0.063 - ETA: 4s - loss: 2.2852 - acc: 0.062 - ETA: 4s - loss: 2.2853 - acc: 0.061 - ETA: 4s - loss: 2.2876 - acc: 0.060 - ETA: 3s - loss: 2.2848 - acc: 0.067 - ETA: 3s - loss: 2.2847 - acc: 0.066 - ETA: 3s - loss: 2.2887 - acc: 0.072 - ETA: 3s - loss: 2.2870 - acc: 0.071 - ETA: 3s - loss: 2.2878 - acc: 0.070 - ETA: 3s - loss: 2.2908 - acc: 0.069 - ETA: 3s - loss: 2.2908 - acc: 0.075 - ETA: 3s - loss: 2.2910 - acc: 0.074 - ETA: 2s - loss: 2.2917 - acc: 0.073 - ETA: 2s - loss: 2.2915 - acc: 0.072 - ETA: 2s - loss: 2.2914 - acc: 0.071 - ETA: 2s - loss: 2.2916 - acc: 0.070 - ETA: 2s - loss: 2.2917 - acc: 0.069 - ETA: 2s - loss: 2.2907 - acc: 0.075 - ETA: 2s - loss: 2.2912 - acc: 0.074 - ETA: 2s - loss: 2.2920 - acc: 0.073 - ETA: 2s - loss: 2.2926 - acc: 0.072 - ETA: 1s - loss: 2.2915 - acc: 0.077 - ETA: 1s - loss: 2.2905 - acc: 0.076 - ETA: 1s - loss: 2.2869 - acc: 0.081 - ETA: 1s - loss: 2.2933 - acc: 0.080 - ETA: 1s - loss: 2.2943 - acc: 0.079 - ETA: 1s - loss: 2.2957 - acc: 0.078 - ETA: 1s - loss: 2.2962 - acc: 0.077 - ETA: 1s - loss: 2.2955 - acc: 0.076 - ETA: 0s - loss: 2.2943 - acc: 0.076 - ETA: 0s - loss: 2.2936 - acc: 0.075 - ETA: 0s - loss: 2.2960 - acc: 0.074 - ETA: 0s - loss: 2.2965 - acc: 0.073 - ETA: 0s - loss: 2.2981 - acc: 0.072 - ETA: 0s - loss: 2.2981 - acc: 0.072 - ETA: 0s - loss: 2.2984 - acc: 0.071 - ETA: 0s - loss: 2.2978 - acc: 0.0758Epoch 00005: val_loss did not improve\n",
      "100/100 [==============================] - 12s - loss: 2.2986 - acc: 0.0750 - val_loss: 2.3081 - val_acc: 0.1000\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 11s - loss: 2.2647 - acc: 0.50 - ETA: 11s - loss: 2.1876 - acc: 0.25 - ETA: 11s - loss: 2.2089 - acc: 0.33 - ETA: 11s - loss: 2.1690 - acc: 0.37 - ETA: 10s - loss: 2.2004 - acc: 0.30 - ETA: 10s - loss: 2.2291 - acc: 0.25 - ETA: 10s - loss: 2.2106 - acc: 0.21 - ETA: 10s - loss: 2.2361 - acc: 0.18 - ETA: 10s - loss: 2.2447 - acc: 0.16 - ETA: 10s - loss: 2.2691 - acc: 0.15 - ETA: 10s - loss: 2.2719 - acc: 0.13 - ETA: 10s - loss: 2.2779 - acc: 0.12 - ETA: 10s - loss: 2.2833 - acc: 0.11 - ETA: 9s - loss: 2.2858 - acc: 0.1429 - ETA: 9s - loss: 2.2791 - acc: 0.166 - ETA: 9s - loss: 2.2793 - acc: 0.156 - ETA: 9s - loss: 2.2870 - acc: 0.147 - ETA: 9s - loss: 2.2840 - acc: 0.138 - ETA: 9s - loss: 2.2803 - acc: 0.184 - ETA: 9s - loss: 2.2817 - acc: 0.200 - ETA: 9s - loss: 2.2758 - acc: 0.214 - ETA: 9s - loss: 2.2725 - acc: 0.204 - ETA: 8s - loss: 2.2715 - acc: 0.195 - ETA: 8s - loss: 2.2733 - acc: 0.187 - ETA: 8s - loss: 2.2712 - acc: 0.180 - ETA: 8s - loss: 2.2635 - acc: 0.173 - ETA: 8s - loss: 2.2623 - acc: 0.166 - ETA: 8s - loss: 2.2596 - acc: 0.178 - ETA: 8s - loss: 2.2684 - acc: 0.189 - ETA: 8s - loss: 2.2776 - acc: 0.183 - ETA: 7s - loss: 2.2853 - acc: 0.177 - ETA: 7s - loss: 2.2879 - acc: 0.171 - ETA: 7s - loss: 2.2888 - acc: 0.166 - ETA: 7s - loss: 2.2887 - acc: 0.161 - ETA: 7s - loss: 2.2909 - acc: 0.157 - ETA: 7s - loss: 2.2917 - acc: 0.152 - ETA: 7s - loss: 2.2916 - acc: 0.148 - ETA: 7s - loss: 2.2901 - acc: 0.157 - ETA: 6s - loss: 2.2893 - acc: 0.153 - ETA: 6s - loss: 2.2902 - acc: 0.150 - ETA: 6s - loss: 2.2877 - acc: 0.158 - ETA: 6s - loss: 2.2881 - acc: 0.154 - ETA: 6s - loss: 2.2936 - acc: 0.151 - ETA: 6s - loss: 2.2939 - acc: 0.147 - ETA: 6s - loss: 2.2971 - acc: 0.144 - ETA: 6s - loss: 2.2975 - acc: 0.141 - ETA: 6s - loss: 2.2972 - acc: 0.138 - ETA: 5s - loss: 2.2988 - acc: 0.135 - ETA: 5s - loss: 2.2989 - acc: 0.142 - ETA: 5s - loss: 2.2998 - acc: 0.140 - ETA: 5s - loss: 2.2996 - acc: 0.147 - ETA: 5s - loss: 2.2996 - acc: 0.144 - ETA: 5s - loss: 2.3001 - acc: 0.141 - ETA: 5s - loss: 2.3004 - acc: 0.138 - ETA: 5s - loss: 2.3010 - acc: 0.136 - ETA: 5s - loss: 2.3020 - acc: 0.133 - ETA: 4s - loss: 2.3012 - acc: 0.140 - ETA: 4s - loss: 2.3012 - acc: 0.137 - ETA: 4s - loss: 2.3012 - acc: 0.135 - ETA: 4s - loss: 2.3013 - acc: 0.133 - ETA: 4s - loss: 2.3013 - acc: 0.131 - ETA: 4s - loss: 2.3019 - acc: 0.129 - ETA: 4s - loss: 2.3020 - acc: 0.127 - ETA: 4s - loss: 2.3025 - acc: 0.125 - ETA: 3s - loss: 2.3027 - acc: 0.123 - ETA: 3s - loss: 2.3053 - acc: 0.121 - ETA: 3s - loss: 2.3053 - acc: 0.119 - ETA: 3s - loss: 2.3053 - acc: 0.117 - ETA: 3s - loss: 2.3055 - acc: 0.115 - ETA: 3s - loss: 2.3052 - acc: 0.114 - ETA: 3s - loss: 2.3053 - acc: 0.112 - ETA: 3s - loss: 2.3054 - acc: 0.111 - ETA: 3s - loss: 2.3058 - acc: 0.109 - ETA: 2s - loss: 2.3054 - acc: 0.114 - ETA: 2s - loss: 2.3056 - acc: 0.113 - ETA: 2s - loss: 2.3056 - acc: 0.111 - ETA: 2s - loss: 2.3057 - acc: 0.110 - ETA: 2s - loss: 2.3061 - acc: 0.109 - ETA: 2s - loss: 2.3055 - acc: 0.113 - ETA: 2s - loss: 2.3054 - acc: 0.112 - ETA: 2s - loss: 2.3051 - acc: 0.111 - ETA: 2s - loss: 2.3049 - acc: 0.115 - ETA: 1s - loss: 2.3044 - acc: 0.120 - ETA: 1s - loss: 2.3045 - acc: 0.119 - ETA: 1s - loss: 2.3044 - acc: 0.117 - ETA: 1s - loss: 2.3046 - acc: 0.116 - ETA: 1s - loss: 2.3035 - acc: 0.120 - ETA: 1s - loss: 2.3034 - acc: 0.119 - ETA: 1s - loss: 2.3045 - acc: 0.118 - ETA: 1s - loss: 2.3049 - acc: 0.116 - ETA: 1s - loss: 2.3048 - acc: 0.115 - ETA: 0s - loss: 2.3046 - acc: 0.114 - ETA: 0s - loss: 2.3048 - acc: 0.112 - ETA: 0s - loss: 2.3050 - acc: 0.111 - ETA: 0s - loss: 2.3055 - acc: 0.110 - ETA: 0s - loss: 2.3054 - acc: 0.109 - ETA: 0s - loss: 2.3053 - acc: 0.113 - ETA: 0s - loss: 2.3051 - acc: 0.117 - ETA: 0s - loss: 2.3054 - acc: 0.1162Epoch 00006: val_loss did not improve\n",
      "100/100 [==============================] - 11s - loss: 2.3056 - acc: 0.1150 - val_loss: 2.3017 - val_acc: 0.0800\n",
      "Epoch 8/10\n",
      " 99/100 [============================>.] - ETA: 10s - loss: 2.3185 - acc: 0.0000e+ - ETA: 10s - loss: 2.2941 - acc: 0.0000e+ - ETA: 10s - loss: 2.3003 - acc: 0.0000e+ - ETA: 10s - loss: 2.3031 - acc: 0.0000e+ - ETA: 10s - loss: 2.2959 - acc: 0.1000   - ETA: 10s - loss: 2.3003 - acc: 0.08 - ETA: 10s - loss: 2.2990 - acc: 0.07 - ETA: 10s - loss: 2.2981 - acc: 0.06 - ETA: 10s - loss: 2.2980 - acc: 0.05 - ETA: 10s - loss: 2.2983 - acc: 0.05 - ETA: 10s - loss: 2.2955 - acc: 0.09 - ETA: 10s - loss: 2.2975 - acc: 0.08 - ETA: 10s - loss: 2.2910 - acc: 0.11 - ETA: 9s - loss: 2.3004 - acc: 0.1071 - ETA: 9s - loss: 2.2913 - acc: 0.100 - ETA: 9s - loss: 2.2928 - acc: 0.093 - ETA: 9s - loss: 2.2954 - acc: 0.088 - ETA: 9s - loss: 2.2959 - acc: 0.083 - ETA: 9s - loss: 2.2972 - acc: 0.078 - ETA: 9s - loss: 2.2961 - acc: 0.100 - ETA: 8s - loss: 2.2968 - acc: 0.095 - ETA: 8s - loss: 2.2971 - acc: 0.113 - ETA: 8s - loss: 2.2967 - acc: 0.108 - ETA: 8s - loss: 2.2976 - acc: 0.104 - ETA: 8s - loss: 2.2969 - acc: 0.120 - ETA: 8s - loss: 2.2967 - acc: 0.115 - ETA: 8s - loss: 2.2991 - acc: 0.111 - ETA: 8s - loss: 2.2990 - acc: 0.107 - ETA: 7s - loss: 2.2989 - acc: 0.103 - ETA: 7s - loss: 2.3012 - acc: 0.100 - ETA: 7s - loss: 2.3018 - acc: 0.096 - ETA: 7s - loss: 2.3020 - acc: 0.093 - ETA: 7s - loss: 2.3018 - acc: 0.090 - ETA: 7s - loss: 2.2994 - acc: 0.102 - ETA: 7s - loss: 2.3005 - acc: 0.100 - ETA: 7s - loss: 2.3017 - acc: 0.097 - ETA: 7s - loss: 2.3034 - acc: 0.094 - ETA: 6s - loss: 2.3037 - acc: 0.105 - ETA: 6s - loss: 2.3034 - acc: 0.102 - ETA: 6s - loss: 2.3078 - acc: 0.100 - ETA: 6s - loss: 2.3057 - acc: 0.097 - ETA: 6s - loss: 2.3050 - acc: 0.095 - ETA: 6s - loss: 2.3061 - acc: 0.093 - ETA: 6s - loss: 2.3078 - acc: 0.090 - ETA: 6s - loss: 2.3075 - acc: 0.088 - ETA: 6s - loss: 2.3074 - acc: 0.087 - ETA: 5s - loss: 2.3074 - acc: 0.085 - ETA: 5s - loss: 2.3087 - acc: 0.083 - ETA: 5s - loss: 2.3084 - acc: 0.081 - ETA: 5s - loss: 2.3086 - acc: 0.080 - ETA: 5s - loss: 2.3090 - acc: 0.078 - ETA: 5s - loss: 2.3088 - acc: 0.086 - ETA: 5s - loss: 2.3095 - acc: 0.084 - ETA: 5s - loss: 2.3097 - acc: 0.083 - ETA: 5s - loss: 2.3096 - acc: 0.081 - ETA: 4s - loss: 2.3089 - acc: 0.089 - ETA: 4s - loss: 2.3093 - acc: 0.087 - ETA: 4s - loss: 2.3087 - acc: 0.086 - ETA: 4s - loss: 2.3060 - acc: 0.093 - ETA: 4s - loss: 2.3012 - acc: 0.091 - ETA: 4s - loss: 2.2985 - acc: 0.098 - ETA: 4s - loss: 2.3076 - acc: 0.096 - ETA: 4s - loss: 2.3108 - acc: 0.095 - ETA: 4s - loss: 2.3112 - acc: 0.093 - ETA: 3s - loss: 2.3117 - acc: 0.092 - ETA: 3s - loss: 2.3119 - acc: 0.090 - ETA: 3s - loss: 2.3117 - acc: 0.089 - ETA: 3s - loss: 2.3123 - acc: 0.088 - ETA: 3s - loss: 2.3121 - acc: 0.087 - ETA: 3s - loss: 2.3124 - acc: 0.085 - ETA: 3s - loss: 2.3125 - acc: 0.084 - ETA: 3s - loss: 2.3125 - acc: 0.083 - ETA: 3s - loss: 2.3125 - acc: 0.082 - ETA: 2s - loss: 2.3120 - acc: 0.087 - ETA: 2s - loss: 2.3108 - acc: 0.093 - ETA: 2s - loss: 2.3107 - acc: 0.092 - ETA: 2s - loss: 2.3106 - acc: 0.090 - ETA: 2s - loss: 2.3103 - acc: 0.089 - ETA: 2s - loss: 2.3103 - acc: 0.088 - ETA: 2s - loss: 2.3106 - acc: 0.087 - ETA: 2s - loss: 2.3108 - acc: 0.086 - ETA: 2s - loss: 2.3111 - acc: 0.085 - ETA: 1s - loss: 2.3107 - acc: 0.084 - ETA: 1s - loss: 2.3094 - acc: 0.089 - ETA: 1s - loss: 2.3115 - acc: 0.088 - ETA: 1s - loss: 2.3111 - acc: 0.087 - ETA: 1s - loss: 2.3111 - acc: 0.086 - ETA: 1s - loss: 2.3114 - acc: 0.085 - ETA: 1s - loss: 2.3112 - acc: 0.084 - ETA: 1s - loss: 2.3113 - acc: 0.083 - ETA: 1s - loss: 2.3111 - acc: 0.082 - ETA: 0s - loss: 2.3106 - acc: 0.081 - ETA: 0s - loss: 2.3105 - acc: 0.080 - ETA: 0s - loss: 2.3101 - acc: 0.085 - ETA: 0s - loss: 2.3108 - acc: 0.084 - ETA: 0s - loss: 2.3117 - acc: 0.083 - ETA: 0s - loss: 2.3118 - acc: 0.082 - ETA: 0s - loss: 2.3113 - acc: 0.081 - ETA: 0s - loss: 2.3112 - acc: 0.0808Epoch 00007: val_loss did not improve\n",
      "100/100 [==============================] - 11s - loss: 2.3113 - acc: 0.0800 - val_loss: 2.3022 - val_acc: 0.0600\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 10s - loss: 2.3517 - acc: 0.0000e+ - ETA: 10s - loss: 2.3136 - acc: 0.2500   - ETA: 10s - loss: 2.2927 - acc: 0.33 - ETA: 10s - loss: 2.2962 - acc: 0.25 - ETA: 10s - loss: 2.2789 - acc: 0.30 - ETA: 10s - loss: 2.2859 - acc: 0.25 - ETA: 10s - loss: 2.2921 - acc: 0.21 - ETA: 10s - loss: 2.2497 - acc: 0.25 - ETA: 10s - loss: 2.2740 - acc: 0.22 - ETA: 10s - loss: 2.2797 - acc: 0.20 - ETA: 9s - loss: 2.2763 - acc: 0.1818 - ETA: 9s - loss: 2.2598 - acc: 0.208 - ETA: 9s - loss: 2.2610 - acc: 0.192 - ETA: 9s - loss: 2.2357 - acc: 0.214 - ETA: 9s - loss: 2.2401 - acc: 0.200 - ETA: 9s - loss: 2.2137 - acc: 0.250 - ETA: 9s - loss: 2.2495 - acc: 0.235 - ETA: 9s - loss: 2.2692 - acc: 0.222 - ETA: 8s - loss: 2.2635 - acc: 0.210 - ETA: 8s - loss: 2.2688 - acc: 0.200 - ETA: 8s - loss: 2.2672 - acc: 0.190 - ETA: 8s - loss: 2.2685 - acc: 0.204 - ETA: 8s - loss: 2.2595 - acc: 0.217 - ETA: 8s - loss: 2.2673 - acc: 0.208 - ETA: 8s - loss: 2.2665 - acc: 0.200 - ETA: 8s - loss: 2.2625 - acc: 0.211 - ETA: 7s - loss: 2.2625 - acc: 0.222 - ETA: 7s - loss: 2.2602 - acc: 0.214 - ETA: 7s - loss: 2.2616 - acc: 0.206 - ETA: 7s - loss: 2.2592 - acc: 0.200 - ETA: 7s - loss: 2.2583 - acc: 0.193 - ETA: 7s - loss: 2.2804 - acc: 0.187 - ETA: 7s - loss: 2.2818 - acc: 0.181 - ETA: 7s - loss: 2.2874 - acc: 0.176 - ETA: 7s - loss: 2.2828 - acc: 0.171 - ETA: 7s - loss: 2.2860 - acc: 0.180 - ETA: 6s - loss: 2.2865 - acc: 0.175 - ETA: 6s - loss: 2.2905 - acc: 0.171 - ETA: 6s - loss: 2.2906 - acc: 0.166 - ETA: 6s - loss: 2.2922 - acc: 0.162 - ETA: 6s - loss: 2.2915 - acc: 0.158 - ETA: 6s - loss: 2.2907 - acc: 0.154 - ETA: 6s - loss: 2.2923 - acc: 0.151 - ETA: 6s - loss: 2.2922 - acc: 0.159 - ETA: 6s - loss: 2.2909 - acc: 0.155 - ETA: 5s - loss: 2.2916 - acc: 0.152 - ETA: 5s - loss: 2.2916 - acc: 0.148 - ETA: 5s - loss: 2.2927 - acc: 0.145 - ETA: 5s - loss: 2.2929 - acc: 0.142 - ETA: 5s - loss: 2.2936 - acc: 0.140 - ETA: 5s - loss: 2.2934 - acc: 0.137 - ETA: 5s - loss: 2.2955 - acc: 0.134 - ETA: 5s - loss: 2.2964 - acc: 0.132 - ETA: 5s - loss: 2.2958 - acc: 0.138 - ETA: 4s - loss: 2.2970 - acc: 0.136 - ETA: 4s - loss: 2.2985 - acc: 0.133 - ETA: 4s - loss: 2.2982 - acc: 0.140 - ETA: 4s - loss: 2.2991 - acc: 0.137 - ETA: 4s - loss: 2.2995 - acc: 0.135 - ETA: 4s - loss: 2.2993 - acc: 0.133 - ETA: 4s - loss: 2.2997 - acc: 0.131 - ETA: 4s - loss: 2.2993 - acc: 0.129 - ETA: 4s - loss: 2.2993 - acc: 0.127 - ETA: 3s - loss: 2.2996 - acc: 0.125 - ETA: 3s - loss: 2.3001 - acc: 0.123 - ETA: 3s - loss: 2.2998 - acc: 0.128 - ETA: 3s - loss: 2.2998 - acc: 0.126 - ETA: 3s - loss: 2.2995 - acc: 0.132 - ETA: 3s - loss: 2.2997 - acc: 0.130 - ETA: 3s - loss: 2.2995 - acc: 0.128 - ETA: 3s - loss: 2.2998 - acc: 0.126 - ETA: 3s - loss: 2.2999 - acc: 0.125 - ETA: 2s - loss: 2.3000 - acc: 0.123 - ETA: 2s - loss: 2.2998 - acc: 0.128 - ETA: 2s - loss: 2.2999 - acc: 0.133 - ETA: 2s - loss: 2.3001 - acc: 0.131 - ETA: 2s - loss: 2.2998 - acc: 0.136 - ETA: 2s - loss: 2.2996 - acc: 0.134 - ETA: 2s - loss: 2.2990 - acc: 0.139 - ETA: 2s - loss: 2.2982 - acc: 0.137 - ETA: 2s - loss: 2.3004 - acc: 0.135 - ETA: 1s - loss: 2.3006 - acc: 0.134 - ETA: 1s - loss: 2.3014 - acc: 0.132 - ETA: 1s - loss: 2.3010 - acc: 0.131 - ETA: 1s - loss: 2.3018 - acc: 0.129 - ETA: 1s - loss: 2.3016 - acc: 0.127 - ETA: 1s - loss: 2.3012 - acc: 0.132 - ETA: 1s - loss: 2.3018 - acc: 0.130 - ETA: 1s - loss: 2.3017 - acc: 0.129 - ETA: 1s - loss: 2.3022 - acc: 0.127 - ETA: 0s - loss: 2.3023 - acc: 0.126 - ETA: 0s - loss: 2.3023 - acc: 0.125 - ETA: 0s - loss: 2.3023 - acc: 0.123 - ETA: 0s - loss: 2.3018 - acc: 0.122 - ETA: 0s - loss: 2.3016 - acc: 0.121 - ETA: 0s - loss: 2.3017 - acc: 0.119 - ETA: 0s - loss: 2.3018 - acc: 0.118 - ETA: 0s - loss: 2.3012 - acc: 0.117 - ETA: 0s - loss: 2.3016 - acc: 0.1162Epoch 00008: val_loss improved from 2.30152 to 2.29440, saving model to saved_models/weights.best.from_scratch.real.aug.v9.hdf5\n",
      "100/100 [==============================] - 11s - loss: 2.3014 - acc: 0.1150 - val_loss: 2.2944 - val_acc: 0.1600\n",
      "Epoch 10/10\n",
      " 99/100 [============================>.] - ETA: 10s - loss: 2.3754 - acc: 0.0000e+ - ETA: 11s - loss: 2.3317 - acc: 0.0000e+ - ETA: 11s - loss: 2.3150 - acc: 0.0000e+ - ETA: 11s - loss: 2.3221 - acc: 0.0000e+ - ETA: 11s - loss: 2.3177 - acc: 0.0000e+ - ETA: 10s - loss: 2.3137 - acc: 0.0000e+ - ETA: 10s - loss: 2.2983 - acc: 0.0714   - ETA: 10s - loss: 2.2950 - acc: 0.06 - ETA: 10s - loss: 2.2903 - acc: 0.05 - ETA: 10s - loss: 2.2942 - acc: 0.10 - ETA: 10s - loss: 2.2950 - acc: 0.09 - ETA: 9s - loss: 2.2986 - acc: 0.0833 - ETA: 9s - loss: 2.3010 - acc: 0.076 - ETA: 9s - loss: 2.3006 - acc: 0.071 - ETA: 9s - loss: 2.2957 - acc: 0.100 - ETA: 9s - loss: 2.2882 - acc: 0.093 - ETA: 9s - loss: 2.2902 - acc: 0.088 - ETA: 9s - loss: 2.2951 - acc: 0.083 - ETA: 9s - loss: 2.2897 - acc: 0.105 - ETA: 8s - loss: 2.2904 - acc: 0.125 - ETA: 8s - loss: 2.2953 - acc: 0.119 - ETA: 8s - loss: 2.2915 - acc: 0.113 - ETA: 8s - loss: 2.2928 - acc: 0.130 - ETA: 8s - loss: 2.2942 - acc: 0.125 - ETA: 8s - loss: 2.2882 - acc: 0.140 - ETA: 8s - loss: 2.2860 - acc: 0.134 - ETA: 8s - loss: 2.2821 - acc: 0.148 - ETA: 8s - loss: 2.2680 - acc: 0.160 - ETA: 8s - loss: 2.2675 - acc: 0.155 - ETA: 8s - loss: 2.2678 - acc: 0.166 - ETA: 7s - loss: 2.2859 - acc: 0.161 - ETA: 7s - loss: 2.2905 - acc: 0.156 - ETA: 7s - loss: 2.2882 - acc: 0.181 - ETA: 7s - loss: 2.2890 - acc: 0.176 - ETA: 7s - loss: 2.2898 - acc: 0.171 - ETA: 7s - loss: 2.2887 - acc: 0.180 - ETA: 7s - loss: 2.2853 - acc: 0.189 - ETA: 7s - loss: 2.2873 - acc: 0.184 - ETA: 7s - loss: 2.2870 - acc: 0.179 - ETA: 6s - loss: 2.2867 - acc: 0.175 - ETA: 6s - loss: 2.2866 - acc: 0.170 - ETA: 6s - loss: 2.2872 - acc: 0.166 - ETA: 6s - loss: 2.2858 - acc: 0.174 - ETA: 6s - loss: 2.2864 - acc: 0.170 - ETA: 6s - loss: 2.2868 - acc: 0.177 - ETA: 6s - loss: 2.2868 - acc: 0.173 - ETA: 6s - loss: 2.2875 - acc: 0.170 - ETA: 5s - loss: 2.2891 - acc: 0.166 - ETA: 5s - loss: 2.2879 - acc: 0.173 - ETA: 5s - loss: 2.2884 - acc: 0.180 - ETA: 5s - loss: 2.2888 - acc: 0.176 - ETA: 5s - loss: 2.2889 - acc: 0.173 - ETA: 5s - loss: 2.2886 - acc: 0.169 - ETA: 5s - loss: 2.2887 - acc: 0.175 - ETA: 5s - loss: 2.2882 - acc: 0.181 - ETA: 5s - loss: 2.2858 - acc: 0.187 - ETA: 4s - loss: 2.2848 - acc: 0.184 - ETA: 4s - loss: 2.2850 - acc: 0.181 - ETA: 4s - loss: 2.2825 - acc: 0.178 - ETA: 4s - loss: 2.2838 - acc: 0.175 - ETA: 4s - loss: 2.2836 - acc: 0.172 - ETA: 4s - loss: 2.2876 - acc: 0.169 - ETA: 4s - loss: 2.2879 - acc: 0.166 - ETA: 4s - loss: 2.2882 - acc: 0.164 - ETA: 4s - loss: 2.2888 - acc: 0.161 - ETA: 3s - loss: 2.2890 - acc: 0.159 - ETA: 3s - loss: 2.2893 - acc: 0.156 - ETA: 3s - loss: 2.2884 - acc: 0.154 - ETA: 3s - loss: 2.2849 - acc: 0.152 - ETA: 3s - loss: 2.2862 - acc: 0.157 - ETA: 3s - loss: 2.2896 - acc: 0.154 - ETA: 3s - loss: 2.2893 - acc: 0.152 - ETA: 3s - loss: 2.2890 - acc: 0.157 - ETA: 2s - loss: 2.2890 - acc: 0.155 - ETA: 2s - loss: 2.2900 - acc: 0.153 - ETA: 2s - loss: 2.2899 - acc: 0.157 - ETA: 2s - loss: 2.2904 - acc: 0.155 - ETA: 2s - loss: 2.2897 - acc: 0.160 - ETA: 2s - loss: 2.2891 - acc: 0.158 - ETA: 2s - loss: 2.2917 - acc: 0.156 - ETA: 2s - loss: 2.2951 - acc: 0.154 - ETA: 2s - loss: 2.2952 - acc: 0.152 - ETA: 1s - loss: 2.2955 - acc: 0.156 - ETA: 1s - loss: 2.2955 - acc: 0.154 - ETA: 1s - loss: 2.2956 - acc: 0.152 - ETA: 1s - loss: 2.2953 - acc: 0.157 - ETA: 1s - loss: 2.2961 - acc: 0.155 - ETA: 1s - loss: 2.2962 - acc: 0.153 - ETA: 1s - loss: 2.2963 - acc: 0.151 - ETA: 1s - loss: 2.2959 - acc: 0.155 - ETA: 1s - loss: 2.2954 - acc: 0.153 - ETA: 0s - loss: 2.2956 - acc: 0.152 - ETA: 0s - loss: 2.2959 - acc: 0.150 - ETA: 0s - loss: 2.2958 - acc: 0.154 - ETA: 0s - loss: 2.2965 - acc: 0.152 - ETA: 0s - loss: 2.2967 - acc: 0.151 - ETA: 0s - loss: 2.2968 - acc: 0.149 - ETA: 0s - loss: 2.2967 - acc: 0.153 - ETA: 0s - loss: 2.2969 - acc: 0.1515Epoch 00009: val_loss did not improve\n",
      "100/100 [==============================] - 12s - loss: 2.2968 - acc: 0.1500 - val_loss: 2.3045 - val_acc: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29b48a576d8>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "epochs = 10\n",
    "batchSize = 2\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.real.aug.v9.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "model.fit_generator(train_data_generator.flow(train_tensors, train_targets, batch_size=batchSize),\n",
    "                    steps_per_epoch=len(train_tensors) / batchSize, \n",
    "                    epochs=epochs, callbacks=[checkpointer],\n",
    "                    validation_data=valid_data_generator.flow(valid_tensors, valid_targets, batch_size=batchSize),\n",
    "                    #validation_data=(valid_tensors, valid_targets),\n",
    "                    validation_steps=len(valid_tensors) // batchSize,\n",
    "                    verbose=1, workers=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.real.aug.v9.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 6.0000%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
